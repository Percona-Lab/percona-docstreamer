# -----------------------------------------------
# docStreamer Configuration File
# -----------------------------------------------
# This file is watched by the application at runtime.
# Changes will be logged, and new settings will be
# picked up by new processes.
#
# Environment variables will OVERRIDE these settings.
# e.g., export DOCDB_ENDPOINT=other.host
# -----------------------------------------------

# -----------------------------------------------
# Logging Configuration
# -----------------------------------------------
logging:
  # Log level (e.g., "debug", "info", "warn", "error")
  level: "info"
 
  # Path to the log file.
  # The directory will be created if it doesn't exist.
  file_path: "logs/docStreamer.log"
 
  # Path to CDC ops log
  ops_log_path: "logs/cdc.log"
 
  # Path to full load ops log
  full_load_log_path: "logs/full_load.log"

# -----------------------------------------------
# Source DocumentDB
# -----------------------------------------------
docdb:
  endpoint: "localhost"
  port: "7777"
  tls: true
  ca_file: "/home/daniel.almeida/global-bundle.pem"
  # If true, tlsAllowInvalidHostnames=true will be added to the connection string.
  tls_allow_invalid_hostnames: true
  # extra_params: "directConnection=true&replicaSet=rsName"
  extra_params: ""

# -----------------------------------------------
# Target MongoDB
# -----------------------------------------------
mongo:
  endpoint: "dan-ps-lab-mongos00.tp.int.percona.com"
  port: "27017"
  tls: false
  ca_file: ""
  tls_allow_invalid_hostnames: true 
  # extra_params: "directConnection=true&replicaSet=rsName"
  extra_params: ""

# -----------------------------------------------
# General Migration Settings
# -----------------------------------------------
migration:
  # Prefix for environment variables (e.g., MIGRATION_DOCDB_USER)
  env_prefix: "MIGRATION"
  pid_file_path: "docStreamer.pid"
  # Port for the HTTP /status endpoint
  status_http_port: "8080"
  network_compressors: "zlib,snappy"
  
  # Databases to skip during discovery
  exclude_dbs:
    - "admin"
    - "local"
    - "config"
 
  # Collections to skip during discovery (format: "dbname.collname")
  # If you want to skip all collections for a given database, use exclude_dbs setting instead
  exclude_collections: []  
 
  # max_concurrent_workers: This is the max number of collections to copy at the same time
  # Controls how many collections are migrated simultaneously during the full load stage
  # Example: If you have 50 collections and set this to 2, docStreamer will migrate 2 collections at a time.
  # As soon as one finishes, the next one starts.
  max_concurrent_workers: 2
 
  # Database and collections to keep track of the migration
  metadata_db: "docStreamer"
  checkpoint_collection: "checkpoints"
  checkpoint_doc_id: "cdc_resume_timestamp"
  status_collection: "status"
  status_doc_id: "migration_status"
  validation_stats_collection: "validation_stats"
  validation_failures_collection: "validation_failures"
 
  # Set the following to True if you want to start the migration all over from scratch
  # This will drop all databases and collections in the destination environment
  # except for admin, local and config
  destroy: False
 
  # Set the following to True if you do not want to make any changes and just want to validate you are able to connect 
  # to source and destination
  dry_run: False

sharding:
  # Example 1: Compound Range Sharding (Default behavior)
  # Shards on 'email' (ascending) and 'rental_id' (ascending)
  # - namespace: "my_db.users"
  #   shard_key: "email, rental_id"
  - namespace: "lukRange_1.test_1"
    shard_key: "uuid_type:1, objectid_type:1"
    pre_split_strategy: "composite_uuid_oid"
    # Explicitly map the fields (REQUIRED)
    # These must match the field names in your shard_key above
    uuid_field: "uuid_type"
    oid_field: "objectid_type"

  # # Example 2: Hashed Sharding
  # # Shards on 'uuid' using a Hashed Index
  # - namespace: "my_db.orders"
  #   shard_key: "uuid:hashed"

  # # Example 3: Compound Hashed Sharding
  # # Shards on 'region' (Range) and 'user_id' (Hashed)
  # - namespace: "my_db.audit_logs"
  #   shard_key: "region:1, user_id:hashed"  


# -----------------------------------------------
# Online Data Validation Settings
# -----------------------------------------------
validation:
  # Enabled: If true, every batch written by CDC is immediately queued for verification.
  # This ensures data consistency in real-time.
  enabled: True
  
  # Batch Size: How many documents to validate in a single lookup operation.
  # Larger batches reduce network round-trips but increase memory usage.
  # Default: 100
  batch_size: 100
  
  # Retry Interval (ms): If a record fails validation because it is being written to (Hot Key),
  # the validator will wait this many milliseconds before checking it again.
  # Default: 500ms
  retry_interval_ms: 500

  # Max Validation Workers: How many concurrent threads to use for verifying data.
  # Increase this to speed up validation if you have spare CPU/Network capacity.
  # Default: 4
  max_validation_workers: 4

  # Max Retries: How many times to retry validation for a "Hot Key" (actively modifying record)
  # before giving up and marking it as skipped/mismatched.
  # Default: 3
  max_retries: 3

  # Queue Size: The buffer size for the validation channel.
  # If the CDC writer is faster than the validator, this buffer fills up.
  # If full, CDC will drop validation requests to avoid slowing down replication.
  # Default: 2000
  queue_size: 2000

# -----------------------------------------------
# Full Load Settings
# -----------------------------------------------
cloner:
  # num_read_workers: Controls how many threads are used to read data for a single collection
  # This is the number of parallel readers (read workers) fetching from DocumentDB for one collection
  num_read_workers: 4
  
  # num_insert_workers: Controls how many threads are used to write data for a single collection
  # This is the number of parallel writers (insert workers) pushing to MongoDB for one given collection 
  # Increase writers to achieve 1:8 ratio with readers, this typically provides an effective performance
  # however it is important to keep in mind this is per collection, so tune this wisely (see max_concurrent_workers)
  # e.g. If num_read_workers = 4 then set num_insert_workers to 32 (e.g. 4 * 8 = 32)
  # CRITICAL: Reduce insert workers to limit concurrent writes.
  # High values here can OOM a shard by flooding it with connections/data.
  # Recommendation: 4-8 per collection
  num_insert_workers: 8
  
  # read_batch_size: Number of documents per read batch
  read_batch_size: 1000
  
  # Reduce batch size to lower memory pressure per request on the cluster.
  # insert_batch_size: Number of documents per insert batch
  insert_batch_size: 1000
  
  # insert_batch_bytes: Max size (in bytes) of a single insert batch
  insert_batch_bytes: 50331648 # 48MB

  # segment_size_docs: Size (in docs) of a segment for parallel reads
  # A collection of 1M docs will be split into 100 segments of 10k docs
  segment_size_docs: 10000

  # num_retries: How many times to retry failed operations (reads/writes)
  # Default: 5
  num_retries: 5

  # retry_interval_ms: How long to wait (in ms) between retries
  # Default: 1000
  retry_interval_ms: 1000

  # write_timeout_ms: Max time (in ms) for a single bulk write batch to complete
  # Default: 30000
  write_timeout_ms: 30000

# -----------------------------------------------
# Change Data Capture (CDC) Settings
# -----------------------------------------------
cdc:
  # batch_size: How many operations to batch together
  batch_size: 1000
  
  # batch_interval_ms: Max time to wait (in ms) before flushing a batch
  # This ensures low-volume changes are still applied quickly
  batch_interval_ms: 500
  
  # max_await_time_ms: Max time (in ms) for the change stream to wait for new events
  max_await_time_ms: 1000
  
  # max_write_workers: Controls concurrency during the cdc phase
  # After the full load finishes, this setting controls how many threads apply the stream of real-time changes.
  # This determines how "wide" your write pipeline is during the live cdc phase. A higher number allows for more parallelism when replaying real-time events
  # Increase this value to utilize more target MongoDB resources and improve real-time throughput. (Default: 8)
  # Increasing this value helps if you have a very high volume of changes on the source (DocumentDB) and your target (MongoDB) has plenty of CPU/IO capacity. 
  # It prevents the application from falling behind simply because it can't write fast enough.
  # Resource Usage: Setting this too high can saturate the connections or CPU on your target MongoDB cluster, potentially slowing down other operations.
  max_write_workers: 8              

  # num_retries: How many times to retry a failed batch (due to network/connection issues)
  # before giving up and stopping the migration.
  # Default: 10
  num_retries: 10

  # retry_interval_ms: How long to wait (in milliseconds) between retry attempts.
  # Default: 1000 (1 second)
  retry_interval_ms: 1000

  # write_timeout_ms: The maximum time to wait for a BulkWrite operation to complete.
  # If the network hangs, this ensures the worker doesn't freeze forever.
  # Default: 30000 (30 seconds)
  write_timeout_ms: 30000

# -----------------------------------------------
# Adaptive Flow Control (Throttling)
# -----------------------------------------------
flow_control:
  # Enabled: If true, docStreamer continuously monitors the target MongoDB's health.
  # It polls `db.serverStatus()` on the target (and all shards if discovered) every second.
  # If any node shows signs of overload (high queues or memory usage), 
  # docStreamer will temporarily pause fetching new data from the source.
  enabled: true

  # Check Interval: How often (in milliseconds) to poll the target database for its status.
  # Default: 1000 (1 second)
  check_interval_ms: 1000

  # Target Max Queued Ops: The safety limit for the Target's Global Lock Queue.
  # Source Metric: db.serverStatus().globalLock.currentQueue.total
  # Checks BOTH Mongos and all backend Shards.
  # If ANY node has more than this many operations queued, docStreamer pauses.
  # Default: 50
  target_max_queued_ops: 50

  # Target Max Resident MB: The safety limit for Target RAM usage.
  # Source Metric: db.serverStatus().mem.resident
  # If the target's Resident Memory exceeds this value (in Megabytes), docStreamer will pause. 
  # This is critical for preventing OOM (Out of Memory) kills on the target host.
  # SHARDED CLUSTERS NOTE: 
  # If the migration user exists on the backend shards, this setting PROTECTS THE SHARDS directly.
  # If the user only exists on Mongos, this setting only protects the Mongos router.
  # Set to 0 to disable memory-based throttling.
  # Default: 0 (Disabled)
  target_max_resident_mb: 0

  # Pause Duration: How long (in milliseconds) to sleep when an overload is detected.
  # The application will re-check the status after this duration.
  # Default: 500
  pause_duration_ms: 500